{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pp68FAQf9aMN"
   },
   "source": [
    "# Sarcasm Detection\n",
    " **Acknowledgement**\n",
    "\n",
    "Misra, Rishabh, and Prahal Arora. \"Sarcasm Detection using Hybrid Neural Network.\" arXiv preprint arXiv:1908.07414 (2019).\n",
    "\n",
    "**Required Files given in below link.**\n",
    "\n",
    "https://drive.google.com/drive/folders/1xUnF35naPGU63xwRDVGc-DkZ3M8V5mMk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S3Wj_mIZ8S3K"
   },
   "source": [
    "## Install `Tensorflow2.0` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jW2Uk8otQvi8"
   },
   "outputs": [],
   "source": [
    "# !!pip uninstall tensorflow\n",
    "# !pip install tensorflow==2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v9kv9tyJ77eF"
   },
   "source": [
    "## Get Required Files from Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "D0O_n6OIEVyL",
    "outputId": "3d02fc9c-2fb9-4e05-d74a-a0f457b5e601"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "\n",
    "#using local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0mgRpOvFMjKR"
   },
   "outputs": [],
   "source": [
    "#Set your project path \n",
    "# project_path =  ## Add your path here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXYwajPeQbRq"
   },
   "source": [
    "#**## Reading and Exploring Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vAk6BRUh8CqL"
   },
   "source": [
    "## Read Data \"Sarcasm_Headlines_Dataset.json\". Explore the data and get  some insights about the data. ( 4 marks)\n",
    "Hint - As its in json format you need to use pandas.read_json function. Give paraemeter lines = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "StSLB-T8PuGr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10297</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/one-size-...</td>\n",
       "      <td>one size does not fit all: three questions to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26300</th>\n",
       "      <td>https://www.theonion.com/bubba-gump-shrimp-own...</td>\n",
       "      <td>bubba gump shrimp owner comforts depressed guy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20641</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/tpp-signe...</td>\n",
       "      <td>u.s. allies sign landmark trade pact as trump ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>https://entertainment.theonion.com/new-documen...</td>\n",
       "      <td>new documentary focuses on life of eva braun's...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571</th>\n",
       "      <td>https://local.theonion.com/newlyweds-regret-sa...</td>\n",
       "      <td>newlyweds regret saving sex for marriage</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            article_link  \\\n",
       "10297  https://www.huffingtonpost.com/entry/one-size-...   \n",
       "26300  https://www.theonion.com/bubba-gump-shrimp-own...   \n",
       "20641  https://www.huffingtonpost.com/entry/tpp-signe...   \n",
       "5565   https://entertainment.theonion.com/new-documen...   \n",
       "1571   https://local.theonion.com/newlyweds-regret-sa...   \n",
       "\n",
       "                                                headline  is_sarcastic  \n",
       "10297  one size does not fit all: three questions to ...             0  \n",
       "26300  bubba gump shrimp owner comforts depressed guy...             1  \n",
       "20641  u.s. allies sign landmark trade pact as trump ...             0  \n",
       "5565   new documentary focuses on life of eva braun's...             1  \n",
       "1571            newlyweds regret saving sex for marriage             1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarcasm_df = pd.read_json('./Sarcasm_Headlines_Dataset.json', lines=True)\n",
    "sarcasm_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z6pXf7A78E2H"
   },
   "source": [
    "## Drop `article_link` from dataset. ( 2 marks)\n",
    "As we only need headline text data and is_sarcastic column for this project. We can drop artical link column here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLSVsvrlP9qD"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  is_sarcastic\n",
       "0  former versace store clerk sues over secret 'b...             0\n",
       "1  the 'roseanne' revival catches up to our thorn...             0\n",
       "2  mom starting to fear son's web series closest ...             1\n",
       "3  boehner just wants wife to listen, not come up...             1\n",
       "4  j.k. rowling wishes snape happy birthday in th...             0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarcasm_df.drop('article_link', axis=1, inplace=True)\n",
    "sarcasm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0h6IOxU8OdH"
   },
   "source": [
    "## Get the Length of each line and find the maximum length. ( 4 marks)\n",
    "As different lines are of different length. We need to pad the our sequences using the max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BRAsChZAQmr3"
   },
   "outputs": [],
   "source": [
    "sarcasm_df['len'] = sarcasm_df['headline'].apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  is_sarcastic  len\n",
       "0  former versace store clerk sues over secret 'b...             0   12\n",
       "1  the 'roseanne' revival catches up to our thorn...             0   14\n",
       "2  mom starting to fear son's web series closest ...             1   14\n",
       "3  boehner just wants wife to listen, not come up...             1   13\n",
       "4  j.k. rowling wishes snape happy birthday in th...             0   11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarcasm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headline        â€‹report: all standing between trump and presid...\n",
       "is_sarcastic                                                    1\n",
       "len                                                            39\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarcasm_df.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VPPd0YuPXi2M"
   },
   "source": [
    "#**## Modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35abKfRx8as3"
   },
   "source": [
    "## Import required modules required for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DVel73hYEV4r"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ziybaD1RdD9"
   },
   "source": [
    "# Set Different Parameters for the model. ( 2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jPw9gAN_EV6m"
   },
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "maxlen = 30\n",
    "embedding_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9abSe-bM8fn9"
   },
   "source": [
    "## Apply Keras Tokenizer of headline column of your data.  ( 4 marks)\n",
    "Hint - First create a tokenizer instance using Tokenizer(num_words=max_features) \n",
    "And then fit this tokenizer instance on your data column df['headline'] using .fit_on_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T9Ad26HfTFMS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 26709\n",
      "[307, 678, 3336, 2297, 47, 381, 2575, 5, 2576, 8433]\n",
      "\n",
      "Number of Labels:  26709\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(sarcasm_df['headline'])\n",
    "X = tokenizer.texts_to_sequences(sarcasm_df['headline'])\n",
    "\n",
    "print(\"Number of Samples:\", len(X))\n",
    "print(X[0])\n",
    "\n",
    "X = pad_sequences(X, maxlen = maxlen)\n",
    "y = np.asarray(sarcasm_df['is_sarcastic'])\n",
    "\n",
    "print(\"\\nNumber of Labels: \", len(y))\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ffi63KsST3P"
   },
   "source": [
    "# Define X and y for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wnjxBdqmSS4s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 26709\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0  307  678 3336 2297   47  381 2575    5\n",
      " 2576 8433]\n",
      "Number of Labels:  26709\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(sarcasm_df['headline'])\n",
    "X = pad_sequences(X, maxlen = maxlen)\n",
    "y = np.asarray(sarcasm_df['is_sarcastic'])\n",
    "\n",
    "print(\"Number of Samples:\", len(X))\n",
    "print(X[0])\n",
    "print(\"Number of Labels: \", len(y))\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJLyKg-98rH_"
   },
   "source": [
    "## Get the Vocabulary size ( 2 marks)\n",
    "Hint : You can use tokenizer.word_index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-2w0gHEUUIo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29657"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = len(tokenizer.word_index) + 1\n",
    "num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5hjeMi40XcB1"
   },
   "source": [
    "#**## Word Embedding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bUF1TuQa8ux0"
   },
   "source": [
    "## Get Glove Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vq5AIfRtMeZh"
   },
   "outputs": [],
   "source": [
    "glove_file = \"./glove.6B.200d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJLX_n2WMecA"
   },
   "outputs": [],
   "source": [
    "#Extract Glove embedding zip file\n",
    "# from zipfile import ZipFile\n",
    "# with ZipFile(glove_file, 'r') as z:\n",
    "#   z.extractall()\n",
    "\n",
    "#Downloaded flove.6B.100d only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9IuXlu8-U3HG"
   },
   "source": [
    "# Get the Word Embeddings using Embedding file as given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elZ-T5aFGZmZ"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = glove_file\n",
    "\n",
    "embeddings = {}\n",
    "for o in open(EMBEDDING_FILE, encoding=\"utf8\"):\n",
    "    word = o.split(\" \")[0]\n",
    "    # print(word)\n",
    "    embd = o.split(\" \")[1:]\n",
    "    embd = np.asarray(embd, dtype='float32')\n",
    "    # print(embd)\n",
    "    embeddings[word] = embd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bTPxveDmVCrA"
   },
   "source": [
    "# Create a weight matrix for words in training docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xQgOhiywU9nU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((num_words, 200))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "len(embeddings.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.6792e-03,  2.2325e-01, -9.7926e-02, -1.6128e-01,  4.7453e-01,\n",
       "       -3.3332e-01, -3.7491e-01, -4.1808e-02, -5.9711e-02,  2.3397e-01,\n",
       "        5.7158e-01,  2.8719e-01, -1.1798e-01,  3.5308e-01,  2.7206e-01,\n",
       "        7.1822e-03, -3.8106e-01,  3.5700e-01,  1.6333e-01,  3.2810e-01,\n",
       "       -1.9585e-02,  2.8545e+00,  3.0997e-01, -1.7071e-01,  6.5618e-01,\n",
       "        6.3599e-01,  2.2558e-01, -3.7270e-02,  3.6916e-01,  2.1133e-01,\n",
       "       -2.0398e-01, -2.2599e-01, -3.5113e-04, -2.6588e-01, -1.8939e-01,\n",
       "       -4.1834e-01, -4.7140e-01, -4.1733e-01,  2.7964e-01, -2.0345e-01,\n",
       "       -1.1666e-01, -1.9084e-02, -2.4930e-02,  1.5921e-01, -5.8741e-02,\n",
       "       -1.5579e-01,  3.0610e-01, -3.6300e-01,  8.4587e-02, -2.3893e-02,\n",
       "       -1.2391e-01,  3.3058e-01, -3.2631e-01,  5.6357e-01,  1.5621e-01,\n",
       "       -3.0395e-01,  3.0185e-01,  1.8804e-01,  2.7050e-01, -8.5709e-03,\n",
       "        7.3487e-02,  1.4172e-01, -5.5930e-01, -1.2523e-01, -5.2305e-01,\n",
       "        2.2663e-01,  2.3772e-02,  3.1973e-01, -2.3190e-01,  2.1769e-01,\n",
       "        5.2629e-01, -2.4725e-01,  3.4529e-01,  4.2877e-01,  7.8787e-05,\n",
       "       -5.4810e-02, -1.4262e-01, -3.2847e-01, -1.1109e-01, -7.9276e-02,\n",
       "        8.2449e-01,  1.1542e-01, -4.6650e-01,  3.3753e-01,  4.5258e-01,\n",
       "       -1.3418e-01, -3.9014e-01,  8.8605e-02,  1.0323e+00, -9.4124e-01,\n",
       "        6.9720e-01, -4.1201e-01, -1.4959e-01,  1.5979e-01, -4.5486e-01,\n",
       "        1.8562e-01,  2.1658e-01, -1.3463e-01,  4.9553e-02,  8.3637e-02,\n",
       "       -8.7120e-02, -1.9525e-01, -2.7269e-01,  2.6005e-01, -1.6372e-02,\n",
       "        3.3542e-01, -2.5220e-02,  1.0853e+00, -3.5816e-01, -2.9095e-01,\n",
       "        1.2773e-01, -6.7995e-01,  2.4261e-01, -1.5214e-01,  3.5039e-02,\n",
       "       -3.2179e-01, -9.4791e-02,  1.0433e-01, -4.1849e-01, -3.4484e-01,\n",
       "        3.2968e-01, -2.9631e-01,  3.6257e-01,  2.4945e-02, -1.4248e-01,\n",
       "       -1.5088e-01,  7.0787e-01,  1.3119e-02,  5.0401e-03, -5.1618e-01,\n",
       "        5.6760e-02,  4.6107e-01,  3.8550e-02, -5.1247e-01, -5.0198e-02,\n",
       "        4.3918e-01,  3.2069e-01,  2.5641e-02, -3.0582e-01, -3.6473e-04,\n",
       "       -2.0451e-01, -3.1046e-01, -3.1095e-01, -1.2046e-01,  1.3697e+00,\n",
       "        3.9866e-01, -7.6891e-02, -5.2583e-01, -2.0430e-02, -9.0804e-02,\n",
       "        5.9366e-01,  3.1014e-01, -7.0257e-02, -6.5326e-01,  3.4809e-02,\n",
       "       -6.9142e-01, -2.4899e-01, -2.0927e-01, -4.7195e-02,  3.5105e-01,\n",
       "        4.7450e-01,  5.5277e-02,  2.0737e-01, -1.5987e-01, -1.7818e-01,\n",
       "        2.3228e-01, -1.5886e-01, -1.6294e-01,  6.6815e-02,  6.4801e-01,\n",
       "       -2.2142e-01,  2.8220e-01,  4.7239e-02,  1.5179e-01,  4.2934e-01,\n",
       "       -2.5535e-01, -7.2418e-01, -2.4960e-01,  5.4558e-01,  2.4107e-01,\n",
       "        9.5550e-01, -1.1706e-01, -8.8162e-03, -7.2432e-02,  1.9031e-01,\n",
       "        1.0898e-01, -1.4574e-01, -2.8885e-01,  8.3009e-02,  4.4254e-01,\n",
       "       -5.5800e-01,  5.4495e-02, -3.7817e-01, -7.8310e-02,  5.1383e-01,\n",
       "       -3.8200e-01, -4.6936e-02,  3.9707e-01,  7.8734e-02,  2.3301e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check embedding for word ==> work\n",
    "embeddings['work']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u7IbWuEX82Ra"
   },
   "source": [
    "## Create and Compile your Model  ( 7 marks)\n",
    "Hint - Use Sequential model instance and then add Embedding layer, Bidirectional(LSTM) layer, then dense and dropout layers as required. \n",
    "In the end add a final dense layer with sigmoid activation for binary classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d7jhsSgYXG4l"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_size, weights = [embedding_matrix]))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences = True)))\n",
    "model.add(Dense(40, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IJFMxZwMWoTw"
   },
   "source": [
    "# Fit your model with a batch size of 100 and validation_split = 0.2. and state the validation accuracy ( 5 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZpVkajCcWnRK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21367 samples, validate on 5342 samples\n",
      "Epoch 1/5\n",
      "21367/21367 [==============================] - 18s 861us/sample - loss: 0.4717 - accuracy: 0.7795 - val_loss: 0.4083 - val_accuracy: 0.8069\n",
      "Epoch 2/5\n",
      "21367/21367 [==============================] - 13s 620us/sample - loss: 0.2827 - accuracy: 0.8920 - val_loss: 0.3497 - val_accuracy: 0.8592\n",
      "Epoch 3/5\n",
      "21367/21367 [==============================] - 13s 585us/sample - loss: 0.1922 - accuracy: 0.9309 - val_loss: 0.3787 - val_accuracy: 0.8593\n",
      "Epoch 4/5\n",
      "21367/21367 [==============================] - 13s 587us/sample - loss: 0.1343 - accuracy: 0.9529 - val_loss: 0.4935 - val_accuracy: 0.8600\n",
      "Epoch 5/5\n",
      "21367/21367 [==============================] - 13s 605us/sample - loss: 0.0897 - accuracy: 0.9683 - val_loss: 0.7336 - val_accuracy: 0.8541\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 5\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 5\n",
    "history = model.fit(X, y, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29657, 200)\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in tokenizer.word_index.items()])\n",
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_n in range(1, num_words):\n",
    "    word = reverse_word_index[word_n]\n",
    "    embeddings1 = weights[word_n]\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in embeddings1]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use these tsv file to visualize the on https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Sarcasm_Detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
